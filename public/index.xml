<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>John Kane</title>
    <link>/</link>
    <description>Recent content on John Kane</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 01 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Visualizing Gradient Boosting Decision Trees for Classification</title>
      <link>/blog/visualizing-gradient-boosting/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/visualizing-gradient-boosting/</guid>
      <description>Introduction Ensemble methods (random forests, gradient boosting machines) have proven to be a winning strategy on Kaggle1. The building blocks of those methods are decision trees, which are generally well understood. It can seem, however, that the ensembling of many trees can produce a sort of magic that allows it to achieve much better performance than a single tree.
When learning about these methods the discussion moves quickly from declaring them an ensemble of trees into a discussion of hyperparameter tuning, glossing over exactly how or why boosting works so well.</description>
    </item>
    
    <item>
      <title>NCAA Men&#39;s Tournament First Round Upsets</title>
      <link>/blog/ncaa-first-round-upsets/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/ncaa-first-round-upsets/</guid>
      <description>Unless you follow mid-major conferences closely, when it comes time to picking first round upsets in the NCAA Tournament you’re likely either picking teams at random, or going off the analysis of the “experts” on TV, the internet, or podcasts. I won’t address those who pick their upsets and Cinderella teams at random here, instead this post is about those who combine whatever they know about the field in a given year with what they see, read, and hear in the media.</description>
    </item>
    
    <item>
      <title>R Functions and Environments</title>
      <link>/blog/functions-and-environments/</link>
      <pubDate>Sun, 11 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/functions-and-environments/</guid>
      <description>The content of this post nearly became a Stack Overflow question. In the process of going about making a reproducible example I solved the problem, as often happens. It took a bit of time to stich together several other SO posts before I arrived at it. Hopefully when someone else comes across the issue maybe this will provide some immediate assistance.
The following is the reprex I went about writing to include in the Stack Overflow post.</description>
    </item>
    
    <item>
      <title>Snake Draft Assistant</title>
      <link>/projects/snake-assistant/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/projects/snake-assistant/</guid>
      <description>This application recommends selections by position based on how much value they add to your team compared to the players likely to be available the next to time you select.
It does this by analyzing:
Predicted points per game of every player Your roster Average draft position  Jump to the app here or read the analytical details below.
Predicted Points Per Game Many websites produce preseason forecasts of player PPG.</description>
    </item>
    
  </channel>
</rss>