<!DOCTYPE html>
<html lang="en-US">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="author" content="John C. Kane">
<meta name="description" content="knitr::opts_chunk$get(&#39;fig.path&#39;) ## [1] &quot;2019-01-21-visualizing-gradient-boosting_files/figure-html/&quot; Introduction Gradient boosting has proven to be a popular and successful learning algorightm in both machine learning competitions and in production commercially. (LINKS for both).
Given the success the algroithm as experienced, several years ago when I was learning about the algorithm I came across a (post)[] outlining how boosting gradually moves closer to the goal output.
Gradient boosted trees provide these advantages along with the advantage of capturing non-linear relationships among the variables.">

<meta property="og:title" content="Visualizing Gradient Boosting for Classification" />
<meta property="og:description" content="knitr::opts_chunk$get(&#39;fig.path&#39;) ## [1] &quot;2019-01-21-visualizing-gradient-boosting_files/figure-html/&quot; Introduction Gradient boosting has proven to be a popular and successful learning algorightm in both machine learning competitions and in production commercially. (LINKS for both).
Given the success the algroithm as experienced, several years ago when I was learning about the algorithm I came across a (post)[] outlining how boosting gradually moves closer to the goal output.
Gradient boosted trees provide these advantages along with the advantage of capturing non-linear relationships among the variables." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/visualizing-gradient-boosting/" />



<meta property="article:published_time" content="2019-02-10T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-02-10T00:00:00&#43;00:00"/>












<title>


     Visualizing Gradient Boosting for Classification 

</title>
<link rel="canonical" href="/post/visualizing-gradient-boosting/">







<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">




<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500">



    
    <link rel="stylesheet" href="/css/reset.css?t=2019-02-23%2015%3a27%3a48.04799566%20-0600%20CST%20m%3d%2b0.027862540">
    <link rel="stylesheet" href="/css/pygments.css?t=2019-02-23%2015%3a27%3a48.04799566%20-0600%20CST%20m%3d%2b0.027862540">
    <link rel="stylesheet" href="/css/main.css?t=2019-02-23%2015%3a27%3a48.04799566%20-0600%20CST%20m%3d%2b0.027862540">
    
        <link rel="stylesheet" href="/css/override.css?t=2019-02-23%2015%3a27%3a48.04799566%20-0600%20CST%20m%3d%2b0.027862540">
    




<link rel="shortcut icon"

    href="/img/jkane-78color"

>








</head>


<body lang="en">

<section class="header">
    <div class="container">
        <div class="content">
            
                
                
                
                
                
                    
                
                    
                
                    
                
                    
                
                    
                
                
                <a href="/"><img class="avatar" src="/img/jkane-78color.jpg" srcset="/img/jkane-78color.jpg 1x"></a>
            
            <a href="/"><div class="name">John C. Kane</div></a>
            
            <nav>
                <ul>
                    
                        <li class="nav-blog"><a href="/blog/"><span>Blog</span></a></li>
                    
                        <li class="nav-projects"><a href="/projects/"><span>Projects</span></a></li>
                    
                        <li class="nav-about"><a href="/about/"><span>About</span></a></li>
                    
                </ul>
            </nav>
        </div>
    </div>
</section>

<section class="icons">
    <div class="container">
        <div class="content">

        
            <a href="//github.com/johnckane" target="_blank" rel="noopener"><img class="icon" src="/img/github.svg" alt="github" /></a>
        

        

        

        

        
            <a href="//www.linkedin.com/in/johnckane1" target="_blank" rel="noopener"><img class="icon" src="/img/linkedin.svg" alt="linkedin" /></a>
        

        

        
            
        

        

        
            <a href="mailto:johnckane1@gmail.com"><img class="icon" src="/img/email.svg" alt="email" /></a>
        

        
            <a href="/index.xml"><img class="icon" src="/img/rss.svg" alt="rss" /></a>
        
        
        </div>
    </div>
</section>

<section class="main">
    <div class="container">
        <div class="content">
            <div class="page-heading">

    Visualizing Gradient Boosting for Classification

</div>

            <div class="markdown">
                <pre class="r"><code>knitr::opts_chunk$get(&#39;fig.path&#39;)</code></pre>
<pre><code>## [1] &quot;2019-01-21-visualizing-gradient-boosting_files/figure-html/&quot;</code></pre>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Gradient boosting has proven to be a popular and successful learning algorightm in both machine learning competitions and in production commercially. (LINKS for both).</p>
<p>Given the success the algroithm as experienced, several years ago when I was learning about the algorithm I came across a (post)[] outlining how boosting gradually moves closer to the goal output.</p>
<p>Gradient boosted trees provide these advantages along with the advantage of capturing non-linear relationships among the variables.</p>
<p>In this post I highlight the algorithm, write code to fit this model, and apply it to a dataset where the output is a non-linear function of the input.</p>
</div>
<div id="algorithm" class="section level2">
<h2>Algorithm</h2>
<p>In <em>An Introduction to Statistical Learning</em> by James, Witten, Hastie, and Tibshirani they lay out the boosting algorithm on page 322:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\hat{f}(x)\)</span> = 0 and <span class="math inline">\(r_{i}\)</span> = <span class="math inline">\(y_{i}\)</span> for all <span class="math inline">\(i\)</span> in the training set.</li>
<li>For <span class="math inline">\(b\)</span> = 1,2,…,B, repeat:
<ol style="list-style-type: lower-alpha">
<li>Fit a tree <span class="math inline">\(\hat{f}^{b}\)</span> with d splits (d+1 terminal nodes) to the training data (X,r).</li>
<li>Update <span class="math inline">\(\hat{f}\)</span> by adding in a shrunken version of the new tree: <span class="math inline">\(\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f}^{b}(x)\)</span>.</li>
<li>Update the residuals, <span class="math inline">\(r_{i}\leftarrow r_{i} - \lambda\hat{f}^{b}(x_{i})\)</span>.</li>
</ol></li>
<li>Output the boosted model, <span class="math inline">\(\hat{f}(x) = \sum_{b=1}^{B}\lambda \hat{f}^{b}(x)\)</span></li>
</ol>
</div>
<div id="my-paraphrasing-of-the-the-alrgorithm-without-the-mathematical-notation" class="section level2">
<h2>My Paraphrasing of the the Alrgorithm, Without the Mathematical Notation</h2>
<ol style="list-style-type: decimal">
<li>Set all predictions to 0, and calculate the residuals (will be the value of <span class="math inline">\(y\)</span> since all predictions are 0).</li>
<li>At each of <span class="math inline">\(B\)</span> iterations do:
<ol style="list-style-type: lower-alpha">
<li>Fit a classication tree to the data, mapping the residual, <span class="math inline">\(r\)</span>, from the input, <span class="math inline">\(X\)</span>.</li>
<li>Update your predictions by adding a fraction of what the model predicted.</li>
<li>Update the residuals to reflect the shrunken predictions.</li>
</ol></li>
<li>The final model is the sum of sequentially adding up the predictions from all the fitted trees.</li>
</ol>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>I’ll create a fake dataset with a non-linear relationship between the input and the output. The output <code>y</code> takes on a value of either 0 or 1 based on the value of <code>X</code>.</p>
<pre class="r"><code>X &lt;- seq(from = 0, to = 100, by = 1)
y &lt;- ifelse(X &lt;= 15,1,
            ifelse(X &lt;= 25,0,
                   ifelse(X &lt;= 45,1,
                          ifelse(X &lt;= 70,0,
                                 ifelse(X &lt;= 95,1,0)))))
plot(x = X, y = y,pch = 20)</code></pre>
<p><img src="/post/2019-01-21-visualizing-gradient-boosting_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="code" class="section level2">
<h2>Code</h2>
<ol start="0" style="list-style-type: decimal">
<li>Load libraries, instantiate R objects to store intermediate results and set hyperparameters.</li>
</ol>
<pre class="r"><code>library(tibble)
library(ggplot2)
library(gganimate)
library(rpart)
trees &lt;- list()
predictions &lt;- list()
losses &lt;- numeric()
logloss_func &lt;- function(y,yhat){
  sum(-y*log(yhat+1e-8) + (1-yhat)*log(1-yhat+1e-8)) # add stability to log calculation
}
B = 100
lambda = 0.2</code></pre>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\hat{f}(x)\)</span> = 0 and <span class="math inline">\(r_{i}\)</span> = <span class="math inline">\(y_{i}\)</span> for all <span class="math inline">\(i\)</span> in the training set.</li>
</ol>
<p>We’ll use <code>yhat</code> for predictions and <code>r</code> for residuals.</p>
<pre class="r"><code>data &lt;- tibble(X,y)
data$yhat &lt;- rep(0,times = nrow(data))
data$r &lt;- data$y - data$yhat</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>For <span class="math inline">\(b\)</span> = 1,2,…,B, repeat:
<ol style="list-style-type: lower-alpha">
<li>Fit a tree <span class="math inline">\(\hat{f}^{b}\)</span> with d splits (d+1 terminal nodes) to the training data (X,r).</li>
<li>Update <span class="math inline">\(\hat{f}\)</span> by adding in a shrunken version of the new tree: <span class="math inline">\(\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f^{b}}(x)\)</span>.</li>
<li>Update the residuals, <span class="math inline">\(r_{i}\leftarrow r_{i} - \lambda\hat{f}^{b}(x_{i})\)</span>.</li>
</ol></li>
</ol>
<pre class="r"><code>for(b in 1:B){
  # Part a
  fit &lt;- rpart(r ~ X, data=data, maxdepth = 1, method = &quot;anova&quot;)
  # Part b
  data$yhat &lt;- data$yhat + lambda*predict(fit,newdata=data)
  # Part c
  data$r &lt;- data$r - lambda*predict(fit,newdata=data)
  
  # Save intermediate results for visualization later, and remove current tree
  trees[b] &lt;- list(fit)
  losses &lt;- c(losses,logloss_func(data$y,data$yhat))
  predictions[[b]] &lt;- tibble(X = data$X,
                           yhat = data$yhat)
  rm(fit)
}</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Output the boosted model, <span class="math inline">\(\hat{f}(x) = \sum_{b=1}^{B}\lambda \hat{f}^{b}(x)\)</span></li>
</ol>
<pre class="r"><code>boost_prediction &lt;- function(X,num_trees){
  pred &lt;- numeric(length = length(X))
  for(j in c(1:num_trees)){
    pred &lt;- pred + lambda*predict(trees[[j]],newdata = data.frame(X = X))
    print(pred)
  }
  return(pred)
}</code></pre>
</div>
<div id="visualize" class="section level2">
<h2>Visualize</h2>
<div id="the-loss-function" class="section level4">
<h4>The loss function</h4>
<pre class="r"><code>loss_plot &lt;- ggplot(data = tibble(iteration = c(1:B),
                                  loss = losses),
                    aes(x = iteration, y = loss)) +
  geom_point() +
  geom_line()
loss_plot</code></pre>
<p><img src="/post/2019-01-21-visualizing-gradient-boosting_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="the-boosting-process" class="section level4">
<h4>The boosting process</h4>
<pre class="r"><code>plot_data &lt;- do.call(rbind,predictions)
plot_data$t &lt;- rep(c(1:B), each = 101)
plot_data$y &lt;- rep(data$y,times = B)

p &lt;- ggplot(data = plot_data,
       aes(x = X,y = y, frame = t),
       colour = &#39;black&#39;) +
  geom_point() +
  geom_point(aes(x = X, y = yhat,frame = t), colour = &#39;red&#39;) +
  ggtitle(&quot;Iteration:&quot;)

gganimate(p, interval = 0.1)</code></pre>
<p>–<img src="/home/john/johnckane.com/static/img/boosting.gif" /> <img src="img/boosting.gif" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://freakonometrics.hypotheses.org/19874" class="uri">https://freakonometrics.hypotheses.org/19874</a></p>
<p>James, Gareth Michael., et al. An Introduction to Statistical Learning with Applications in R. Springer, 2013.</p>
</div>
<div id="session-info" class="section level2">
<h2>Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.4 (2018-03-15)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Ubuntu 16.04.4 LTS
## 
## Matrix products: default
## BLAS: /usr/lib/openblas-base/libblas.so.3
## LAPACK: /usr/lib/libopenblasp-r0.2.18.so
## 
## locale:
##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
## [1] rpart_4.1-13         gganimate_0.1.0.9000 ggplot2_3.1.0       
## [4] tibble_1.4.2        
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.19     pillar_1.3.0     compiler_3.4.4   plyr_1.8.4      
##  [5] bindr_0.1.1      base64enc_0.1-3  tools_3.4.4      digest_0.6.18   
##  [9] evaluate_0.12    gtable_0.2.0     pkgconfig_2.0.2  rlang_0.3.0.1   
## [13] yaml_2.2.0       blogdown_0.9     xfun_0.4         bindrcpp_0.2.2  
## [17] withr_2.1.2      stringr_1.3.1    dplyr_0.7.7      knitr_1.20      
## [21] rprojroot_1.3-2  grid_3.4.4       tidyselect_0.2.5 glue_1.3.0      
## [25] R6_2.3.0         animation_2.5    rmarkdown_1.10   bookdown_0.7    
## [29] purrr_0.2.5      magrittr_1.5     backports_1.1.2  scales_1.0.0    
## [33] htmltools_0.3.6  assertthat_0.2.0 colorspace_1.3-2 labeling_0.3    
## [37] stringi_1.2.4    lazyeval_0.2.1   munsell_0.5.0    crayon_1.3.4</code></pre>
</div>

            </div>
        </div>
    </div>
</section>


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-123-45', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  

  <script type="text/javascript">
    hljs.initHighlightingOnLoad();
  </script>





</body>
</html>

