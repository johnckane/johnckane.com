---
title: "Visualizing Gradient Boosting Decision Trees for Classification"
author: "John Kane"
date: '2019-03-27'
slug: visualizing-gradient-boosting
tags:
- machine learning
- gbm
- data visualization
categories:
- machine learning
- gbm
- data visualization
---

## Introduction

Ensemble methods (random forests, gradient boosting machines) have proven to be a winning strategy on Kaggle[^1]. The building blocks of those methods are decision trees, which are generally well understood. It can seem, however, that the ensembling of many trees can produce a sort of magic that allows it to achieve much better performance than a single tree. 

When learning about these methods the discussion moves quickly from declaring them an ensemble of trees into a discussion of hyperparameter tuning, glossing over exactly how or why boosting works so well. When I was first learning the boosting algorithm I came across a blog post by Arthur Charpentier outlining and visualizing in a regression context how boosting gradually moves predictions closer to their target[^2]. Inspired by that post, I've written up this post highlighting the boosting algorithm for classification.

Here I'll highlight the algorithm, write code to fit a model, apply it to a dataset where the output is a non-linear function of the input,visualize the boosting process and associated decrease in the loss function, and briefly discuss implications for cross-validation and model selection in a larger machine learning task. 

## Algorithm

In *An Introduction to Statistical Learning* by James, Witten, Hastie, and Tibshirani they lay out the boosting algorithm on page 322:[^3]

1. Set $\hat{f}(x)$ = 0 and $r_{i}$ = $y_{i}$ for all $i$ in the training set.
2. For $b$ = 1,2,...,B, repeat:
    a. Fit a tree $\hat{f}^{b}$ with d splits (d+1 terminal nodes) to the training data (X,r).
    b. Update $\hat{f}$ by adding in a shrunken version of the new tree:
        $\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f}^{b}(x)$.
    c. Update the residuals,
        $r_{i}\leftarrow r_{i} - \lambda\hat{f}^{b}(x_{i})$.
3. Output the boosted model,
  $\hat{f}(x) = \sum_{b=1}^{B}\lambda \hat{f}^{b}(x)$
  
## Paraphrasing the Algorithm, Without Mathematical Notation

1. Set all predictions to 0, and calculate the residuals (will be the value of $y$ since all predictions are 0).
2. At each of $B$ iterations do:
    a. Fit a classication tree to the data, mapping the residual, $r$, from the input, $X$. 
    b. Update your predictions by adding a fraction of what the model predicted.
    c. Update the residuals to reflect the shrunken predictions.
3. The final model is the sum of sequentially adding up the predictions from all the fitted trees.


## Data

I'll create a fake dataset with a non-linear relationship between the input and the output. The output `y` takes on a value of either 0 or 1 based on the value of `X`.  

```{r}
X <- seq(from = 0, to = 100, by = 1)
y <- ifelse(X <= 15, 1,
            ifelse(X <= 25, 0,
                   ifelse(X <= 45, 1,
                          ifelse(X <= 70, 0,
                                 ifelse(X <= 95, 1, 0)))))
plot(x = X, y = y, pch = 20)
```




## Coding the Algorithm

0. Load libraries, instantiate R objects to store intermediate results, write loss function, and set hyperparameters.
```{r}
library(tibble)
library(rpart)
trees <- list()
predictions <- list()
losses <- numeric()
logloss_func <- function(y, yhat){
  sum(-y*log(yhat+1e-8) + (1-yhat)*log(1-yhat+1e-8)) # add stability to log calculation
}
B = 50
lambda = 0.2
maxdepth = 1
```

1. Set $\hat{f}(x)$ = 0 and $r_{i}$ = $y_{i}$ for all $i$ in the training set.

We'll use `yhat` for predictions and `r` for residuals. 

```{r}
data <- tibble(X, y)
data$yhat <- rep(0, times = nrow(data))
data$r <- data$y - data$yhat
```


2. For $b$ = 1,2,...,B, repeat:
    a. Fit a tree $\hat{f}^{b}$ with d splits (d+1 terminal nodes) to the training data (X,r).
    b. Update $\hat{f}$ by adding in a shrunken version of the new tree:
    $\hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f^{b}}(x)$.
    c. Update the residuals,
    $r_{i}\leftarrow r_{i} - \lambda\hat{f}^{b}(x_{i})$.
    
```{r}
for(b in 1:B){
  # Part a
  fit <- rpart(r ~ X, data=data, maxdepth = maxdepth, method = "anova")
  # Part b
  data$yhat <- data$yhat + lambda*predict(fit,newdata=data)
  # Part c
  data$r <- data$r - lambda*predict(fit,newdata=data)
  
  # Save intermediate results for visualization later, and remove current tree
  trees[b] <- list(fit)
  losses <- c(losses, logloss_func(data$y, data$yhat))
  predictions[[b]] <- tibble(X = data$X,
                           yhat = data$yhat)
  rm(fit)
}
```

3. Output the boosted model,
  $\hat{f}(x) = \sum_{b=1}^{B}\lambda \hat{f}^{b}(x)$
  

```{r}
boost_prediction <- function(X,num_trees){
  pred <- numeric(length = length(X))
  for(j in c(1:num_trees)){
    pred <- pred + lambda*predict(trees[[j]], newdata = data.frame(X = X))
  }
  return(pred)
}
```


## Visualize 

The boosting procedure is an iterative process, which lends itself nicely to visualization. In Step 2 of the code we saved the values of the loss function after every iteration, as well as the fitted values of the model. Here we'll construct a visualization of the process. This visualization is made possible by the fantastic `gganimate` and `magick` packages in R.[^4]

```{r, warning = F, message = F}
library(ggplot2)
library(gganimate)
library(magick)
```


#### Building visualization datasets


```{r}
loss_plot_data <- tibble(iteration = c(1:B),
                                  loss = losses)

boost_plot_data <- do.call(rbind, predictions)
boost_plot_data$B <- rep(c(1:B), each = length(X))
boost_plot_data$y <- rep(data$y, times = B)
```


#### Create the individual plots


```{r}
boost_plot <- ggplot(data = boost_plot_data,
       aes(x = X, y = y),
       colour = 'black') +
  geom_point() +
  geom_point(aes(x = X, y = yhat), colour = 'red') +
  transition_states(B) +
  labs(title = 'Iteration: {closest_state}', subtitle = "Boosting",x = 'X', y = 'y')

boost_gif <- animate(boost_plot, width = 400, height = 400)
```


```{r}
loss_plot <- ggplot(data = loss_plot_data,
                aes(x = iteration, y = loss),
                colour = 'black') +
  geom_point() +
  geom_line() +
  transition_reveal(along = iteration) +
  shadow_trail(distance = 1/B) +
  labs(title = ' ', subtitle = 'Loss')

loss_gif <- animate(loss_plot,width = 400, height = 400)
```


#### Combine the gifs and view

```{r, include = F}
loss_mgif <- image_read(loss_gif)
boost_mgif <- image_read(boost_gif)

combined_gif <- image_append(c(boost_mgif[1], loss_mgif[1]))

for(i in 2:100){
  combined <- image_append(c(boost_mgif[i], loss_mgif[i]))
  combined_gif <- c(combined_gif, combined)
}

magick::image_write(combined_gif, path="/home/john/johnckane.com/static/img/boosting-and-loss.gif")
```

```{r, eval = FALSE}
loss_mgif <- image_read(loss_gif)
boost_mgif <- image_read(boost_gif)

combined_gif <- image_append(c(boost_mgif[1], loss_mgif[1]))

for(i in 2:100){
  combined <- image_append(c(boost_mgif[i], loss_mgif[i]))
  combined_gif <- c(combined_gif, combined)
}

combined_gif
```

![](/home/john/johnckane.com/static/img/boosting-and-loss.gif)

Almost all of the loss reduction occurs in the first 10 iterations even though there isn't separation of all the six groups of $X$ values until the 22nd iteration. Beyond the 22nd iteration we can see improvement in predictions but the changes are slow and gradual, which is also reflected in the loss reduction graph. By the time we get to the 50th iteration we're in a position where if draw our decision boundary to be at $\hat{y} = 0.50$ and predict a value of 1 for all values at or above that line and a value of 0 to all points below that line, that we'd achieve 100% accuracy in the classification task.

## Making predictions and applications to validation and test datasets

To illustrate the sequential nature of making predictions, I'll generate predictions for five $X$ values, and print out the model's predictions using 5, 10, or 50 trees. Generating predictions was Step 3 in the algorithm.

```{r}
vals_to_predict <- c(5,25,35,60,99)
prediction_tibble <- tibble(x = vals_to_predict,
                            y = data$y[vals_to_predict + 1], # the first value of X is 0 so the index of x is x+1
                            five_trees = sapply(vals_to_predict, boost_prediction, num_trees = 5),
                            ten_trees = sapply(vals_to_predict, boost_prediction, num_trees = 10),
                            fifty_trees = sapply(vals_to_predict, boost_prediction, num_trees = 50))
prediction_tibble
```

As seen above, where $y=1$ the predictions follow an upward trajectory of continuous improvement. Where $y=0$ however, there is actually a decrease in accuracy going from 5 trees to 10 before becoming more accurate by the time we reach fifty trees. I think this is a function of the fact that maximum tree depth is set to 1, so that any split that includes a mixture of both 0's and 1's in a terminal node will naturally move predictions upward, away from 0, where all predictions began. Over the course of the boosting process splits will be made where the average value in a terminal node is less than current predictions, which will bring the predictions for values where $y=0$ closer to the true value. 

Before deploying a model it is wise to have an idea of how well it will make predictions on data previously unseen by it. Best practice in this case is to split available data into training, validation, and test datasets. The training dataset is used to fit the model, performance of the model at varying values of hyperparameters is evaluated using the validation dataset, and once a single is model is chosen for deployment, an estimate of model performance on previously unseen data is estimated by using the test dataset. 

It is generally true that model fit metrics strictly improve on the training dataset as model complexity grows. This is not the case with validation datasets. There comes a point the model begins to learn the idiosyncracies of the training data rather than a more general relationship between input features and the output of the data generating mechanism. The appropriate model to use in deployment would be the one that performs the best on the validation dataset, even if it doesn't perform as well on the training dataset. 

Hyperparameter tuning and model selection is not the focus of this post, but several details in this example are relevant to its application. The hyperparameters here are the number of trees ($B$), the value of the learning rate ($\lambda$), and the maximum depth of each tree (1). In this example $\lambda$ and maximum tree depth remained fixed and we iterated our model fitting over $B$ different trees. A fuller treatment would involve fitting multiple models to the training data at varying levels of $\lambda$ and maximum tree depth and comparing model performance on the validation dataset using different number of trees fit (anywhere from $1$ to $B$).

## Session Info
```{r}
sessionInfo()
```

[^1]: https://www.kaggle.com/antgoldbloom/what-algorithms-are-most-successful-on-kaggle

[^2]: https://freakonometrics.hypotheses.org/19874

[^3]: James, Gareth Michael., et al. An Introduction to Statistical Learning with Applications in R. Springer, 2013.

[^4]: https://github.com/thomasp85/gganimate/wiki/Animation-Composition

