{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having recently completed [deeplearning.ai](https://www.coursera.org/specializations/deep-learning)'s I want to continue to build and learn the models. The courses were great, I have a much better understanding about Neural Network architecture as a result of taking them. The programming exercises were good and I feel the course is stuck \n",
    "in between a rock and a hard place. They could either make the programming exercises such that coded needed to be \n",
    "generated from scratch, or they could mock up a skeleon of the program and leave the details to the student. They favored doing the later implementation. As a result when I've gone to sit down and right my own program I'm left scratching my head. \n",
    "\n",
    "When you Google how to do certain things in tensorflow, for example how to get started with writing a program many things surface, the [TensorFlow](https://www.tensorflow.org/) website first among them. The documentation there is \n",
    "great, however when you already know a little about TensorFlow and building Neural Networks, and Google armed with that information, you can find yourself in documentation that you don't want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool concept in fitting Neural Network models is Mini-batch processing. Briefly, rather than using every datapoint in every iteration of feed-forward and back propagation, a smaller subset of the data is used.\n",
    "\n",
    "The idea being that the steps taken during any one iteration may harm the model, in general you will decrease your loss function much more quickly. A surprising thing to me about mini-batch processing is that the sizes of the batches are quite small, compared to the data size as well as my expectations.\n",
    "\n",
    "I would have expected the batches to be on the order of 50% or 10% or 5% of the number of training examples. In practice, however the batch sizes are quite small. Batch sizes of 64, 128, 256 or 512 are quite common. For a dataset with one million training examples that represents a batch size of less than one tenth of one percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data I'll generate some fake data to illustrate the concepts. We'll use 1000 observations, three features (x1, x2, x3) and and output (y) that has a linear relationship with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Set a seed\n",
    "np.random.seed(100)\n",
    "# Create three variables, x1, x2 and x3\n",
    "x1 = 10*np.random.rand(1000)\n",
    "x2 = -3*np.random.rand(1000)\n",
    "x3 = 2*np.random.rand(1000)\n",
    "# make y a linear combination of these variables, plus some noise\n",
    "y = 11 + 1.5*x1 + 6*x2 -3*x3 + np.random.normal(loc = 0, scale = 3, size = 1000)\n",
    "y = y.reshape(1000,1)\n",
    "data_dict = {\"y\": y,\n",
    "            \"x1\": x1,\n",
    "            \"x2\": x2,\n",
    "            \"x3\": x3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level API: Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the recommended program structure for using pre-made estimators from the [TensorFlow](https://www.tensorflow.org/programmers_guide/estimators) website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one or more dataset inputting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data pipleine for this problem is straightforward, so this function is trivial. \n",
    "def data_input(data_dict):\n",
    "     \n",
    "    features = {\"x1\": data_dict[\"x1\"],\n",
    "               \"x2\": data_dict[\"x2\"],\n",
    "               \"x3\": data_dict[\"x3\"]}\n",
    "    \n",
    "    labels = data_dict[\"y\"]\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "features, labels = data_input(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in features.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "print(my_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate the relevant pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = tf.estimator.LinearRegressor(feature_columns = my_feature_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Call a training, evaluation, or inference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.train(input_fn=lambda:data_input(data_dict),steps = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different Approach:\n",
    "#### Import data using Dataset API\n",
    "#### Fit model using Estimator API\n",
    "###### This is what the TensorFlow documents suggest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data into two NumPy arrays, for example using `np.load()`.\n",
    "features = {x_key: data_dict[x_key] for x_key in [\"x1\",\"x2\",\"x3\"]}\n",
    "labels = data_dict[\"y\"]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert features[\"x1\"].shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder1 = tf.placeholder(features[\"x1\"].dtype, features[\"x2\"].shape)\n",
    "features_placeholder2 = tf.placeholder(features[\"x2\"].dtype, features[\"x2\"].shape)\n",
    "features_placeholder3 = tf.placeholder(features[\"x3\"].dtype, features[\"x3\"].shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder1,\n",
    "                                              features_placeholder2,\n",
    "                                              features_placeholder3,                                      \n",
    "                                              labels_placeholder))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={features_placeholder1: features[\"x1\"],\n",
    "                                          features_placeholder2: features[\"x2\"],\n",
    "                                          features_placeholder3: features[\"x3\"],\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this needs to be a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input2(data_dict,num_epochs):\n",
    "    features = {x_key: data_dict[x_key] for x_key in [\"x1\",\"x2\",\"x3\"]}\n",
    "    labels = data_dict[\"y\"]\n",
    "\n",
    "\n",
    "    features_placeholder1 = tf.placeholder(features[\"x1\"].dtype, features[\"x2\"].shape)\n",
    "    features_placeholder2 = tf.placeholder(features[\"x2\"].dtype, features[\"x2\"].shape)\n",
    "    features_placeholder3 = tf.placeholder(features[\"x3\"].dtype, features[\"x3\"].shape)\n",
    "    labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_placeholder1,\n",
    "                                                  features_placeholder2,\n",
    "                                                  features_placeholder3,                                      \n",
    "                                                  labels_placeholder))\n",
    "    dataset.repeat(num_epochs)\n",
    "    #iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    #features, labels = iterator.get_next()\n",
    "    #return features, labels\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examine this more closely:\n",
    "def dataset_input_fn():\n",
    "#filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "#dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "  # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "  # protocol buffer, and perform any additional per-record preprocessing.\n",
    "  def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    #Perform additional preprocessing on the parsed data.\n",
    "    #image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "    #image = tf.reshape(image, [299, 299, 1])\n",
    "    #label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "    return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "  # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "  # tensor for each example.\n",
    "  dataset = dataset.map(parser)\n",
    "  #dataset = dataset.shuffle(buffer_size=10000)\n",
    "  dataset = dataset.batch(32)\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "  # `features` is a dictionary in which each value is a batch of values for\n",
    "  # that feature; `labels` is a batch of labels.\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input3(shape):\n",
    "\n",
    "    features_placeholder1 = tf.placeholder(tf.float32, shape)\n",
    "    features_placeholder2 = tf.placeholder(tf.float32, shape)\n",
    "    features_placeholder3 = tf.placeholder(tf.float32, shape)\n",
    "    labels_placeholder    = tf.placeholder(tf.float32, shape)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_placeholder1,\n",
    "                                                  features_placeholder2,\n",
    "                                                  features_placeholder3,                                      \n",
    "                                                  labels_placeholder))\n",
    "    #def parser():\n",
    "    #    labels = dataset[\"label\"]\n",
    "    #    features = {\"x1\": dataset[\"x1\"],\n",
    "    #                \"x2\": dataset[\"x2\"],\n",
    "    #                \"x3\": dataset[\"x3\"]}\n",
    "    \n",
    "    #dataset = dataset.map(parser)    \n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "#f, l = data_input3(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor2 = tf.estimator.LinearRegressor(feature_columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor2.train(input_fn=lambda:data_input2(data_dict,num_epochs = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once again, I'm trying to combine features I can't.\n",
    "I cannot use:\n",
    "* Dataset API\n",
    "* Placeholders\n",
    "* Input_fn\n",
    "* one_shot_iterator \n",
    "\n",
    "All together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still want to, for this example use:\n",
    "* Dataset API\n",
    "* Estimator API\n",
    "* Dataset input_fun, to feed the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an Input function that uses the dataset API, feed in the actual data, not placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input4(data_dict):\n",
    "    \n",
    "    # Create the dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     feature_dict = {x_key: tf.feature_column.numeric_column(key=x_key,\n",
    "#                                                             dtype=tf.float32,\n",
    "#                                                             default_value=tf.zeros([], dtype=tf.int64)\n",
    "#                                                             shape = data_dict[x_key].shape)\n",
    "\n",
    "#                     for x_key in [\"x1\",\"x2\",\"x3\"]}\n",
    "\n",
    "#     features = {\"x1\": data_dict[\"x1\"],\n",
    "#                \"x2\": data_dict[\"x2\"],\n",
    "#                \"x3\": data_dict[\"x3\"]}\n",
    "    \n",
    "    features = {\"x1\": tf.feature_column.numeric_column(key=\"x1\"),\n",
    "                \"x2\": tf.feature_column.numeric_column(key=\"x2\"),\n",
    "                \"x3\": tf.feature_column.numeric_column(key=\"x3\")\n",
    "               }\n",
    "    \n",
    "#     x1 = tf.feature_column.numeric_column(key=\"x1\")\n",
    "#     x2 = tf.feature_column.numeric_column(key=\"x2\")\n",
    "#     x3 = tf.feature_column.numeric_column(key=\"x3\")\n",
    "    \n",
    "#     features = {\"x1\": x1,\n",
    "#                 \"x2\": x2,\n",
    "#                 \"x3\": x3}\n",
    "    \n",
    "    \n",
    "    \n",
    "#     my_feature_columns = []\n",
    "#     for key in features.keys():\n",
    "#         my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "                                                            \n",
    "                                                            \n",
    "                                                            \n",
    "\n",
    "    #dataset = tf.data.Dataset.from_tensor_slices((dict(my_feature_columns),data_dict[\"y\"]))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,data_dict[\"y\"]))   \n",
    "    \n",
    "\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((data_dict[\"x1\"],\n",
    "#                                                   data_dict[\"x2\"],\n",
    "#                                                   data_dict[\"x3\"],\n",
    "#                                                   data_dict[\"y\"]))\n",
    " \n",
    "  \n",
    "    \n",
    "#     # parse into a features dictionary and labels array\n",
    "#     def parser(d1,d2,d3,d4):\n",
    "        \n",
    "#         keys_to_features = {\n",
    "#         \"x1\":    tf.FixedLenFeature((),tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n",
    "#         \"x2\":    tf.FixedLenFeature((),tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n",
    "#         \"x3\":    tf.FixedLenFeature((),tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n",
    "#         \"label\": tf.FixedLenFeature((),tf.float32,default_value=tf.zeros([], dtype=tf.float32))\n",
    "#         } \n",
    "        \n",
    "#         parsed = tf.parse_single_example(\"\",keys_to_features)\n",
    "        \n",
    "#         x1 = parsed[\"x1\"]\n",
    "#         x2 = parsed[\"x2\"]\n",
    "#         x3 = parsed[\"x3\"]\n",
    "            \n",
    "\n",
    "#         return {\"x1\": x1,\"x2\": x2,\"x3\": x3}, parsed[\"label\"]\n",
    "\n",
    "    \n",
    "#     dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input4(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor4 = tf.estimator.LinearRegressor(feature_columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor4.train(input_fn=lambda:data_input4(data_dict), steps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjtgrctgu\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpjtgrctgu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbc311c0128>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Items of feature_columns must be a _FeatureColumn. Given (type <class 'str'>): x1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7c5531ac3bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#regressor = tf.estimator.LinearRegressor(feature_columns = lambda: input_fn(data_dict)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#I get the following error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    709\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step_read_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 711\u001b[0;31m             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    712\u001b[0m       \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m       \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           config=config)\n\u001b[0m\u001b[1;32m    349\u001b[0m     super(LinearRegressor, self).__init__(\n\u001b[1;32m    350\u001b[0m         \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_linear_model_fn\u001b[0;34m(features, labels, mode, head, feature_columns, optimizer, partitioner, config)\u001b[0m\n\u001b[1;32m    116\u001b[0m     logit_fn = _linear_logit_fn_builder(\n\u001b[1;32m    117\u001b[0m         units=head.logits_dimension, feature_columns=feature_columns)\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_op_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36mlinear_logit_fn\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[1;32m     69\u001b[0m     return feature_column_lib.linear_model(\n\u001b[0;32m---> 70\u001b[0;31m         features=features, feature_columns=feature_columns, units=units)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlinear_logit_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36mlinear_model\u001b[0;34m(features, feature_columns, units, sparse_combiner, weight_collections, trainable)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mnor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_CategoricalColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \"\"\"\n\u001b[0;32m--> 297\u001b[0;31m   \u001b[0mfeature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_feature_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_DenseColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_CategoricalColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_clean_feature_columns\u001b[0;34m(feature_columns)\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_FeatureColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m       raise ValueError('Items of feature_columns must be a _FeatureColumn. '\n\u001b[0;32m-> 1664\u001b[0;31m                        'Given (type {}): {}.'.format(type(column), column))\n\u001b[0m\u001b[1;32m   1665\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_columns must not be empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Items of feature_columns must be a _FeatureColumn. Given (type <class 'str'>): x1."
     ]
    }
   ],
   "source": [
    "## For Stack Overflow:\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(100)\n",
    "x1 = np.random.rand(100)\n",
    "x2 = np.random.rand(100)\n",
    "x3 = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "data_dict = {\"x1\": x1,\n",
    "                 \"x2\": x2,\n",
    "                 \"x3\": x3,\n",
    "                  \"y\": y}\n",
    "\n",
    "features = {\"x1\": x1,\n",
    "           \"x2\": x2,\n",
    "           \"x3\": x3}\n",
    "\n",
    "def input_fn(data_dict):\n",
    "    features = {\"x1\": data_dict[\"x1\"],\n",
    "                \"x2\": data_dict[\"x2\"],\n",
    "                \"x3\": data_dict[\"x3\"]}\n",
    "    \n",
    "#     features = {\"x1\": tf.feature_column.numeric_column(key=\"x1\"),\n",
    "#                 \"x2\": tf.feature_column.numeric_column(key=\"x2\"),\n",
    "#                 \"x3\": tf.feature_column.numeric_column(key=\"x3\")\n",
    "#                }\n",
    "                   \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,data_dict[\"y\"]))   \n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "#regressor = tf.estimator.LinearRegressor(feature_columns = lambda: input_fn(data_dict)[0])\n",
    "regressor = tf.estimator.LinearRegressor(feature_columns = features)\n",
    "regressor.train(input_fn=lambda:input_fn(data_dict), steps = 1)\n",
    "\n",
    "#I get the following error:\n",
    "    \n",
    "#    ValueError: Items of feature_columns must be a _FeatureColumn. Given (type <class 'str'>): x1.\n",
    "\n",
    "#If I replace the `features` definition in `input_fn` by this:\n",
    "\n",
    "#         features = {\"x1\": tf.feature_column.numeric_column(key=\"x1\"),\n",
    "#                 \"x2\": tf.feature_column.numeric_column(key=\"x2\"),\n",
    "#                 \"x3\": tf.feature_column.numeric_column(key=\"x3\")\n",
    "#                }\n",
    "# And execute, I get this error:\n",
    "\n",
    "#     ValueError: None values not supported.\n",
    "\n",
    "# So my question is:\n",
    "\n",
    "# Within the framework of the Dataset and Estimator APIs, how can I use numpy arrays as data, specify feature columns within the `input_fn` and then pass to an Estimator?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmphf3whzk_\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmphf3whzk_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbc30d82470>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature (key: X1) cannot have rank 0. Give: Tensor(\"IteratorGetNext:0\", shape=(), dtype=float64, device=/device:CPU:0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-42d47f8cb458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    709\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step_read_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 711\u001b[0;31m             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    712\u001b[0m       \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m       \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           config=config)\n\u001b[0m\u001b[1;32m    349\u001b[0m     super(LinearRegressor, self).__init__(\n\u001b[1;32m    350\u001b[0m         \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_linear_model_fn\u001b[0;34m(features, labels, mode, head, feature_columns, optimizer, partitioner, config)\u001b[0m\n\u001b[1;32m    116\u001b[0m     logit_fn = _linear_logit_fn_builder(\n\u001b[1;32m    117\u001b[0m         units=head.logits_dimension, feature_columns=feature_columns)\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_op_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36mlinear_logit_fn\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[1;32m     69\u001b[0m     return feature_column_lib.linear_model(\n\u001b[0;32m---> 70\u001b[0;31m         features=features, feature_columns=feature_columns, units=units)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlinear_logit_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36mlinear_model\u001b[0;34m(features, feature_columns, units, sparse_combiner, weight_collections, trainable)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m           weighted_sums.append(_create_dense_column_weighted_sum(\n\u001b[0;32m--> 321\u001b[0;31m               column, builder, units, weight_collections, trainable))\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0m_verify_static_batch_size_equality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     predictions_no_bias = math_ops.add_n(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_create_dense_column_weighted_sum\u001b[0;34m(column, builder, units, weight_collections, trainable)\u001b[0m\n\u001b[1;32m   1364\u001b[0m       \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m       \u001b[0mweight_collections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_collections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m       trainable=trainable)\n\u001b[0m\u001b[1;32m   1367\u001b[0m   \u001b[0mnum_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_get_dense_tensor\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;31m# Feature has been already transformed. Return the intermediate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;31m# representation created by _transform_feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Transforming feature_column %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m     \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Column {} is not supported.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_transform_feature\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_transform_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m       \u001b[0mfeature_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_raw_feature_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_get_raw_feature_as_tensor\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         raise ValueError(\n\u001b[1;32m   1573\u001b[0m             'Feature (key: {}) cannot have rank 0. Give: {}'.format(\n\u001b[0;32m-> 1574\u001b[0;31m                 key, feature_tensor))\n\u001b[0m\u001b[1;32m   1575\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_tensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Feature (key: X1) cannot have rank 0. Give: Tensor(\"IteratorGetNext:0\", shape=(), dtype=float64, device=/device:CPU:0)"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/49088781/tensorflow-valueerror-feature-key-c1-cannot-have-rank-0\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# create data\n",
    "np.random.seed(100)\n",
    "x1 = np.random.rand(100)\n",
    "x2 = np.random.rand(100)\n",
    "x3 = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "#\n",
    "\n",
    "\n",
    "X1 = tf.feature_column.numeric_column(key=\"X1\")\n",
    "X2 = tf.feature_column.numeric_column(key=\"X2\")\n",
    "X3 = tf.feature_column.numeric_column(key=\"X3\")\n",
    "feature_columns = [X1,X2,X3]\n",
    "\n",
    "features = {\"X1\": x1,\n",
    "           \"X2\": x2,\n",
    "           \"X3\": x3}\n",
    "\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices({\"feature_dict\":features,\n",
    "#                                            \"labels\": y})\n",
    "\n",
    "# def input_fn(data):\n",
    "    \n",
    "#     iterator = data.make_one_shot_iterator()\n",
    "#     feature_dict, labels = iterator.get_next()\n",
    "#     return feature_dict, labels\n",
    "\n",
    "\n",
    "def input_fn(features, labels):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Build the Iterator, and return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "\n",
    "\n",
    "regressor = tf.estimator.LinearRegressor(feature_columns = feature_columns)\n",
    "\n",
    "regressor.train(input_fn=lambda:input_fn(features,y), steps = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
